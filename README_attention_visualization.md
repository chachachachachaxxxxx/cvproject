# Vision Transformer æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–å·¥å…·

æœ¬å·¥å…·ä¸“é—¨ç”¨äºå¯è§†åŒ–å’Œåˆ†æVision Transformeråœ¨MNISTæ•°æ®é›†ä¸Šçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¸®åŠ©ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹å’Œç‰¹å¾æå–æ¨¡å¼ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### 1. å¤šå±‚æ¬¡æ³¨æ„åŠ›åˆ†æ
- **å±‚çº§æ³¨æ„åŠ›æ¨¡å¼**: å¯è§†åŒ–æ¯ä¸€å±‚Transformerçš„æ³¨æ„åŠ›åˆ†å¸ƒ
- **æ³¨æ„åŠ›å¤´ä¸“ä¸šåŒ–**: åˆ†æä¸åŒæ³¨æ„åŠ›å¤´çš„åŠŸèƒ½åˆ†å·¥
- **å±‚çº§æ¼”åŒ–å¯¹æ¯”**: å±•ç¤ºæ³¨æ„åŠ›ä»æµ…å±‚åˆ°æ·±å±‚çš„æ¼”åŒ–è¿‡ç¨‹

### 2. ç»Ÿè®¡åˆ†æ
- **æ³¨æ„åŠ›å¼ºåº¦ç»Ÿè®¡**: åˆ†ææœ€å¤§æ³¨æ„åŠ›ã€å¹³å‡æ³¨æ„åŠ›çš„åˆ†å¸ƒ
- **æ³¨æ„åŠ›ç†µè®¡ç®—**: è¡¡é‡æ³¨æ„åŠ›çš„é›†ä¸­ç¨‹åº¦å’Œåˆ†æ•£ç¨‹åº¦
- **è·¨å±‚å¯¹æ¯”**: é‡åŒ–åˆ†æä¸åŒå±‚çš„æ³¨æ„åŠ›ç‰¹å¾

### 3. é«˜è´¨é‡å¯è§†åŒ–
- **çƒ­åŠ›å›¾å åŠ **: åŸå›¾åƒä¸æ³¨æ„åŠ›çƒ­åŠ›å›¾çš„å®Œç¾èåˆ
- **å¤šå¤´å¯¹æ¯”**: å¹¶æ’å±•ç¤ºä¸åŒæ³¨æ„åŠ›å¤´çš„å…³æ³¨ç‚¹
- **ç»Ÿè®¡å›¾è¡¨**: ä¸“ä¸šçš„ç®±å‹å›¾å’Œåˆ†å¸ƒå›¾

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ä¾èµ–åº“
```bash
torch>=1.9.0
torchvision>=0.10.0
matplotlib>=3.3.0
seaborn>=0.11.0
opencv-python>=4.5.0
numpy>=1.19.0
```

### å®‰è£…ä¾èµ–
```bash
pip install torch torchvision matplotlib seaborn opencv-python numpy
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. å¿«é€Ÿå¼€å§‹

```python
from attention_visualizer import AttentionVisualizer

# åˆå§‹åŒ–å¯è§†åŒ–å™¨ï¼ˆä½¿ç”¨æœ€ä¼˜æ¨¡å‹é…ç½®ï¼‰
model_path = "exp/experiments/efficient_ablation_20250609_101146/embed_dim_ablation_embed_dim_128/best_model.pth"
visualizer = AttentionVisualizer(model_path)

# è¿è¡Œå®Œæ•´åˆ†æ
visualizer.run_comprehensive_analysis(model_path)
```

### 2. æµ‹è¯•åŠŸèƒ½
```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python test_attention_visualizer.py
```

### 3. åˆ†æ­¥æ‰§è¡Œ

```python
# åŠ è½½æ¨¡å‹
visualizer.load_optimal_model()

# è·å–æµ‹è¯•æ ·æœ¬
samples = visualizer.get_test_samples(num_samples=5)

# åˆ†æå•ä¸ªæ ·æœ¬
for i, (image, label) in enumerate(samples):
    # åŸºæœ¬æ³¨æ„åŠ›å¯è§†åŒ–
    visualizer.visualize_attention_patterns(image, label, i)
    
    # æ³¨æ„åŠ›å¤´åˆ†æ
    visualizer.analyze_attention_heads(image, label, i)
    
    # å±‚çº§å¯¹æ¯”
    visualizer.create_layer_comparison(image, label, i)

# ç»Ÿè®¡åˆ†æ
visualizer.create_attention_statistics(samples)
```

## ğŸ“Š è¾“å‡ºç»“æœ

### ç”Ÿæˆçš„æ–‡ä»¶ç»“æ„
```
attention_analysis/
â”œâ”€â”€ attention_pattern_sample_0.png      # æ ·æœ¬0çš„æ³¨æ„åŠ›æ¨¡å¼
â”œâ”€â”€ attention_pattern_sample_1.png      # æ ·æœ¬1çš„æ³¨æ„åŠ›æ¨¡å¼
â”œâ”€â”€ ...
â”œâ”€â”€ attention_heads_sample_0.png        # æ ·æœ¬0çš„æ³¨æ„åŠ›å¤´åˆ†æ
â”œâ”€â”€ attention_heads_sample_1.png        # æ ·æœ¬1çš„æ³¨æ„åŠ›å¤´åˆ†æ
â”œâ”€â”€ ...
â”œâ”€â”€ layer_comparison_sample_0.png       # æ ·æœ¬0çš„å±‚çº§å¯¹æ¯”
â”œâ”€â”€ layer_comparison_sample_1.png       # æ ·æœ¬1çš„å±‚çº§å¯¹æ¯”
â”œâ”€â”€ ...
â”œâ”€â”€ attention_statistics.png            # ç»Ÿè®¡åˆ†æå›¾è¡¨
â””â”€â”€ attention_analysis_report.md        # è¯¦ç»†åˆ†ææŠ¥å‘Š
```

### å¯è§†åŒ–å†…å®¹è¯´æ˜

#### 1. æ³¨æ„åŠ›æ¨¡å¼å›¾ (`attention_pattern_sample_*.png`)
- **ä¸Šæ’**: çº¯æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼Œæ˜¾ç¤ºCLS tokenå¯¹å„patchçš„æ³¨æ„åŠ›å¼ºåº¦
- **ä¸‹æ’**: åŸå›¾åƒä¸æ³¨æ„åŠ›çƒ­åŠ›å›¾å åŠ ï¼Œç›´è§‚æ˜¾ç¤ºæ¨¡å‹å…³æ³¨åŒºåŸŸ
- **é¢œè‰²ç¼–ç **: çº¢è‰²è¡¨ç¤ºé«˜æ³¨æ„åŠ›ï¼Œè“è‰²è¡¨ç¤ºä½æ³¨æ„åŠ›

#### 2. æ³¨æ„åŠ›å¤´åˆ†æ (`attention_heads_sample_*.png`)
- å±•ç¤º4ä¸ªæ³¨æ„åŠ›å¤´çš„ä¸åŒå…³æ³¨æ¨¡å¼
- æ¯ä¸ªå¤´å¯èƒ½ä¸“æ³¨äºä¸åŒçš„ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€å½¢çŠ¶ã€çº¹ç†ç­‰ï¼‰
- å¸®åŠ©ç†è§£å¤šå¤´æ³¨æ„åŠ›çš„åˆ†å·¥æœºåˆ¶

#### 3. å±‚çº§æ¼”åŒ–å¯¹æ¯” (`layer_comparison_sample_*.png`)
- æ˜¾ç¤º6ä¸ªTransformerå±‚çš„æ³¨æ„åŠ›æ¼”åŒ–
- å¯è§‚å¯Ÿåˆ°ä»å±€éƒ¨åˆ°å…¨å±€çš„ç‰¹å¾æå–è¿‡ç¨‹
- å±•ç¤ºæ¨¡å‹çš„åˆ†å±‚æŠ½è±¡èƒ½åŠ›

#### 4. ç»Ÿè®¡åˆ†æ (`attention_statistics.png`)
- **æœ€å¤§æ³¨æ„åŠ›**: æ¯å±‚æœ€å¼ºæ³¨æ„åŠ›å€¼çš„åˆ†å¸ƒ
- **å¹³å‡æ³¨æ„åŠ›**: æ¯å±‚å¹³å‡æ³¨æ„åŠ›å¼ºåº¦
- **æ³¨æ„åŠ›ç†µ**: æ³¨æ„åŠ›åˆ†æ•£ç¨‹åº¦çš„é‡åŒ–æŒ‡æ ‡

## ğŸ” ä¸»è¦å‘ç°

åŸºäºå®éªŒç»“æœï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä»¥ä¸‹å…³é”®æ¨¡å¼ï¼š

### 1. å±‚çº§ç‰¹å¾æå–
- **æµ…å±‚ (Layer 1-2)**: å…³æ³¨å±€éƒ¨è¾¹ç¼˜å’Œç»†èŠ‚ç‰¹å¾
- **ä¸­å±‚ (Layer 3-4)**: å¼€å§‹æ•´åˆå±€éƒ¨ç‰¹å¾ï¼Œå½¢æˆéƒ¨åˆ†ç»“æ„ç†è§£
- **æ·±å±‚ (Layer 5-6)**: å…³æ³¨å…¨å±€å½¢çŠ¶å’Œå®Œæ•´æ•°å­—ç»“æ„

### 2. æ³¨æ„åŠ›å¤´ä¸“ä¸šåŒ–
- **å¤´1**: é€šå¸¸å…³æ³¨æ•°å­—çš„ä¸»ä½“è½®å»“
- **å¤´2**: ä¸“æ³¨äºç»†èŠ‚ç‰¹å¾å’Œç¬”ç”»è¿æ¥
- **å¤´3**: å…³æ³¨èƒŒæ™¯å’Œè¾¹ç•Œä¿¡æ¯
- **å¤´4**: æ•´åˆå…¨å±€ä¿¡æ¯è¿›è¡Œæœ€ç»ˆåˆ†ç±»

### 3. æ•°å­—ç‰¹å¾è¯†åˆ«
- æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«æ•°å­—çš„å…³é”®ç‰¹å¾ç‚¹
- å¯¹äºä¸åŒæ•°å­—ç±»åˆ«ï¼Œæ³¨æ„åŠ›æ¨¡å¼æœ‰æ˜æ˜¾å·®å¼‚
- å¼¯æ›²éƒ¨åˆ†ã€äº¤å‰ç‚¹ã€ç«¯ç‚¹ç­‰å…³é”®ç»“æ„è·å¾—æ›´å¤šæ³¨æ„åŠ›

## âš™ï¸ é…ç½®é€‰é¡¹

### æ¨¡å‹é…ç½®ï¼ˆæœ€ä¼˜è®¾ç½®ï¼‰
```python
optimal_config = {
    'img_size': 28,        # å›¾åƒå°ºå¯¸
    'patch_size': 7,       # Patchå¤§å°
    'in_channels': 1,      # è¾“å…¥é€šé“æ•°
    'num_classes': 10,     # ç±»åˆ«æ•°
    'embed_dim': 128,      # åµŒå…¥ç»´åº¦
    'num_heads': 4,        # æ³¨æ„åŠ›å¤´æ•°
    'num_layers': 6,       # Transformerå±‚æ•°
    'mlp_dim': 256,        # MLPç»´åº¦
    'dropout': 0.1         # Dropoutç‡
}
```

### å¯è§†åŒ–å‚æ•°
```python
visualizer = AttentionVisualizer(
    model_path="path/to/model.pth",
    device='cuda'  # æˆ– 'cpu'
)

# è·å–æ ·æœ¬æ•°é‡
samples = visualizer.get_test_samples(num_samples=8)  # é»˜è®¤8ä¸ªæ ·æœ¬
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨`torch.no_grad()`å‡å°‘å†…å­˜å ç”¨
- æ‰¹å¤„ç†å¤§å°è®¾ä¸º1ï¼Œé¿å…å¤§é‡attention mapå ç”¨å†…å­˜
- åŠæ—¶é‡Šæ”¾GPUç¼“å­˜

### è®¡ç®—ä¼˜åŒ–
- æ³¨æ„åŠ›æƒé‡ç›´æ¥ä»æ¨¡å‹è¾“å‡ºè·å–ï¼Œæ— éœ€é¢å¤–è®¡ç®—
- ä½¿ç”¨OpenCVè¿›è¡Œé«˜æ•ˆçš„å›¾åƒä¸Šé‡‡æ ·
- å¹¶è¡Œå¤„ç†å¤šä¸ªæ ·æœ¬çš„å¯è§†åŒ–

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹è·¯å¾„**: ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡
2. **GPUå†…å­˜**: å¤§æ¨¡å‹å¯èƒ½éœ€è¦è¾ƒå¤šGPUå†…å­˜ï¼Œå¯è®¾ç½®`device='cpu'`
3. **ä¾èµ–åº“ç‰ˆæœ¬**: ç¡®ä¿æ‰€æœ‰ä¾èµ–åº“ç‰ˆæœ¬å…¼å®¹
4. **è¾“å‡ºç›®å½•**: ç¨‹åºä¼šè‡ªåŠ¨åˆ›å»º`attention_analysis`ç›®å½•

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹åŠ è½½å¤±è´¥**
   ```
   è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨éšæœºæƒé‡è¿›è¡Œæ¼”ç¤º
   ```

2. **CUDAå†…å­˜ä¸è¶³**
   ```python
   # ä½¿ç”¨CPU
   visualizer = AttentionVisualizer(model_path, device='cpu')
   ```

3. **ä¾èµ–åº“ç¼ºå¤±**
   ```bash
   pip install -r requirements.txt
   ```

### è°ƒè¯•æ¨¡å¼
```python
# å¼€å¯è¯¦ç»†æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æµ‹è¯•å•ä¸ªåŠŸèƒ½
visualizer.visualize_attention_patterns(image, label, 0)
```

## ğŸ“š æ‰©å±•åŠŸèƒ½

### è‡ªå®šä¹‰åˆ†æ
```python
class CustomAttentionVisualizer(AttentionVisualizer):
    def custom_analysis(self, samples):
        # æ·»åŠ è‡ªå®šä¹‰åˆ†æé€»è¾‘
        pass
```

### æ‰¹é‡å¤„ç†
```python
# å¤„ç†å¤šä¸ªæ¨¡å‹
model_paths = ["model1.pth", "model2.pth", "model3.pth"]
for path in model_paths:
    visualizer = AttentionVisualizer(path)
    visualizer.run_comprehensive_analysis(path)
```

## ğŸ“„ è®¸å¯è¯

æœ¬å·¥å…·åŸºäºMITè®¸å¯è¯å¼€æºï¼Œå¯è‡ªç”±ä½¿ç”¨å’Œä¿®æ”¹ã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªå·¥å…·ï¼

---

**ä½œè€…**: AI Research Team  
**æœ€åæ›´æ–°**: 2025å¹´6æœˆ  
**ç‰ˆæœ¬**: 1.0.0 