#!/usr/bin/env python3
"""
æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–å™¨çš„ç®€åŒ–è„šæœ¬
å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡è¿›è¡Œæ¼”ç¤º
"""

import torch
import os
from attention_visualizer import AttentionVisualizer

def test_attention_visualizer():
    """æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–å™¨"""
    print("å¼€å§‹æµ‹è¯•æ³¨æ„åŠ›å¯è§†åŒ–å™¨...")
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ€ä¼˜æ¨¡å‹è·¯å¾„
    optimal_model_path = "exp/experiments/efficient_ablation_20250609_101146/embed_dim_ablation_embed_dim_128/best_model.pth"
    
    if not os.path.exists(optimal_model_path):
        print(f"æœ€ä¼˜æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {optimal_model_path}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¼”ç¤º...")
        model_path = "dummy_model.pth"  # ä½¿ç”¨å ä½ç¬¦è·¯å¾„
    else:
        model_path = optimal_model_path
        print(f"æ‰¾åˆ°æœ€ä¼˜æ¨¡å‹: {model_path}")
    
    try:
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨
        visualizer = AttentionVisualizer(model_path)
        
        # æµ‹è¯•æ¨¡å‹åŠ è½½
        print("\næµ‹è¯•æ¨¡å‹åŠ è½½...")
        visualizer.load_optimal_model()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®è·å–
        print("\næµ‹è¯•æ•°æ®è·å–...")
        samples = visualizer.get_test_samples(num_samples=2)  # åªæµ‹è¯•2ä¸ªæ ·æœ¬
        print(f"âœ“ æˆåŠŸè·å– {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("\næµ‹è¯•å‰å‘ä¼ æ’­å’Œæ³¨æ„åŠ›æå–...")
        sample_image, sample_label = samples[0]
        image_batch = sample_image.unsqueeze(0).to(visualizer.device)
        output, attention_maps = visualizer.forward_with_attention(image_batch)
        
        print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"âœ“ æ³¨æ„åŠ›å±‚æ•°: {len(visualizer.attention_maps)}")
        for layer_name, attention in visualizer.attention_maps.items():
            print(f"  {layer_name}: {attention.shape}")
        
        # æµ‹è¯•å•ä¸ªå¯è§†åŒ–åŠŸèƒ½
        print("\næµ‹è¯•æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–...")
        visualizer.visualize_attention_patterns(sample_image, sample_label, 0)
        print("âœ“ æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–å®Œæˆ")
        
        print("\næµ‹è¯•æ³¨æ„åŠ›å¤´åˆ†æ...")
        visualizer.analyze_attention_heads(sample_image, sample_label, 0)
        print("âœ“ æ³¨æ„åŠ›å¤´åˆ†æå®Œæˆ")
        
        print("\næµ‹è¯•å±‚çº§æ³¨æ„åŠ›å¯¹æ¯”...")
        visualizer.create_layer_comparison(sample_image, sample_label, 0)
        print("âœ“ å±‚çº§æ³¨æ„åŠ›å¯¹æ¯”å®Œæˆ")
        
        print("\næµ‹è¯•ç»Ÿè®¡åˆ†æ...")
        visualizer.create_attention_statistics(samples)
        print("âœ“ ç»Ÿè®¡åˆ†æå®Œæˆ")
        
        print("\næµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
        visualizer._generate_analysis_report(samples)
        print("âœ“ åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»“æœä¿å­˜åœ¨: {visualizer.output_dir}")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        if os.path.exists(visualizer.output_dir):
            for file in os.listdir(visualizer.output_dir):
                file_path = os.path.join(visualizer.output_dir, file)
                if os.path.isfile(file_path):
                    file_size = os.path.getsize(file_path)
                    print(f"  - {file} ({file_size:,} bytes)")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
        
    return True

def run_full_analysis():
    """è¿è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†æ"""
    print("\n" + "="*50)
    print("è¿è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†æ...")
    print("="*50)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    optimal_model_path = "exp/experiments/efficient_ablation_20250609_101146/embed_dim_ablation_embed_dim_128/best_model.pth"
    
    if not os.path.exists(optimal_model_path):
        print(f"æœ€ä¼˜æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {optimal_model_path}")
        print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¼”ç¤º...")
        model_path = "dummy_model.pth"
    else:
        model_path = optimal_model_path
    
    try:
        visualizer = AttentionVisualizer(model_path)
        visualizer.run_comprehensive_analysis(model_path)
        print("\nğŸ‰ å®Œæ•´åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ å®Œæ•´åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
        
    return True

if __name__ == "__main__":
    print("Vision Transformer æ³¨æ„åŠ›å¯è§†åŒ–æµ‹è¯•")
    print("="*50)
    
    # é¦–å…ˆè¿è¡Œå•å…ƒæµ‹è¯•
    if test_attention_visualizer():
        print("\nå•å…ƒæµ‹è¯•é€šè¿‡ï¼Œç»§ç»­è¿è¡Œå®Œæ•´åˆ†æ...")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­è¿è¡Œå®Œæ•´åˆ†æ
        response = input("\næ˜¯å¦è¿è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†æï¼Ÿ(y/n): ").lower().strip()
        if response in ['y', 'yes', 'æ˜¯']:
            run_full_analysis()
        else:
            print("è·³è¿‡å®Œæ•´åˆ†æã€‚")
    else:
        print("\nå•å…ƒæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚") 