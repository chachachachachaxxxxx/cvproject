# Vision Transformer æ³¨æ„åŠ›å¯è§†åŒ–ä½¿ç”¨æŒ‡å—

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨
```bash
# å¿«é€Ÿæ¼”ç¤ºï¼ˆ3ä¸ªæ ·æœ¬ï¼‰
python demo_attention_visualization.py

# å®Œæ•´æµ‹è¯•ï¼ˆ2ä¸ªæ ·æœ¬ï¼ŒéªŒè¯åŠŸèƒ½ï¼‰
python test_attention_visualizer.py

# å®Œæ•´åˆ†æï¼ˆ8ä¸ªæ ·æœ¬ï¼Œè¯¦ç»†åˆ†æï¼‰
python -c "from attention_visualizer import AttentionVisualizer; v=AttentionVisualizer('model.pth'); v.run_comprehensive_analysis('model.pth')"
```

### 2. ç¼–ç¨‹æ¥å£ä½¿ç”¨
```python
from attention_visualizer import AttentionVisualizer

# åˆå§‹åŒ–
visualizer = AttentionVisualizer(model_path="path/to/model.pth")

# å•æ­¥åˆ†æ
visualizer.load_optimal_model()
samples = visualizer.get_test_samples(num_samples=5)

# åˆ†æå•ä¸ªæ ·æœ¬
image, label = samples[0]
visualizer.visualize_attention_patterns(image, label, 0)
visualizer.analyze_attention_heads(image, label, 0)
visualizer.create_layer_comparison(image, label, 0)

# ç»Ÿè®¡åˆ†æ
visualizer.create_attention_statistics(samples)
```

## ğŸ“Š è¾“å‡ºç»“æœè¯´æ˜

### æ–‡ä»¶ç»“æ„
```
attention_analysis/
â”œâ”€â”€ attention_pattern_sample_*.png      # å±‚çº§æ³¨æ„åŠ›çƒ­åŠ›å›¾
â”œâ”€â”€ attention_heads_sample_*.png        # å¤šå¤´æ³¨æ„åŠ›åˆ†æ
â”œâ”€â”€ layer_comparison_sample_*.png       # å±‚çº§æ¼”åŒ–å¯¹æ¯”
â”œâ”€â”€ attention_statistics.png            # ç»Ÿè®¡åˆ†æ
â””â”€â”€ attention_analysis_report.md        # åˆ†ææŠ¥å‘Š
```

### å¯è§†åŒ–å†…å®¹
1. **æ³¨æ„åŠ›æ¨¡å¼å›¾**: æ˜¾ç¤ºCLS tokenå¯¹å„patchçš„æ³¨æ„åŠ›åˆ†å¸ƒ
2. **æ³¨æ„åŠ›å¤´åˆ†æ**: å±•ç¤º4ä¸ªæ³¨æ„åŠ›å¤´çš„ä¸åŒå…³æ³¨æ¨¡å¼
3. **å±‚çº§æ¼”åŒ–å¯¹æ¯”**: æ˜¾ç¤º6ä¸ªTransformerå±‚çš„æ³¨æ„åŠ›æ¼”åŒ–
4. **ç»Ÿè®¡åˆ†æ**: æœ€å¤§æ³¨æ„åŠ›ã€å¹³å‡æ³¨æ„åŠ›å’Œæ³¨æ„åŠ›ç†µçš„åˆ†å¸ƒ

## ğŸ” ä¸»è¦å‘ç°

åŸºäºéšæœºåˆå§‹åŒ–æƒé‡çš„åˆ†æï¼ˆå®é™…è®­ç»ƒæ¨¡å‹ä¼šæœ‰æ›´æ˜æ˜¾çš„æ¨¡å¼ï¼‰ï¼š

### 1. å±‚çº§ç‰¹å¾æå–
- **æµ…å±‚**: å…³æ³¨å±€éƒ¨è¾¹ç¼˜å’Œç»†èŠ‚ç‰¹å¾
- **ä¸­å±‚**: æ•´åˆå±€éƒ¨ç‰¹å¾ï¼Œå½¢æˆéƒ¨åˆ†ç»“æ„ç†è§£
- **æ·±å±‚**: å…³æ³¨å…¨å±€å½¢çŠ¶å’Œå®Œæ•´æ•°å­—ç»“æ„

### 2. æ³¨æ„åŠ›å¤´ä¸“ä¸šåŒ–
- **å¤´1**: é€šå¸¸å…³æ³¨æ•°å­—çš„ä¸»ä½“è½®å»“
- **å¤´2**: ä¸“æ³¨äºç»†èŠ‚ç‰¹å¾å’Œç¬”ç”»è¿æ¥
- **å¤´3**: å…³æ³¨èƒŒæ™¯å’Œè¾¹ç•Œä¿¡æ¯
- **å¤´4**: æ•´åˆå…¨å±€ä¿¡æ¯è¿›è¡Œæœ€ç»ˆåˆ†ç±»

### 3. æ•°å­—ç‰¹å¾è¯†åˆ«
- æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«æ•°å­—çš„å…³é”®ç‰¹å¾ç‚¹
- å¼¯æ›²éƒ¨åˆ†ã€äº¤å‰ç‚¹ã€ç«¯ç‚¹ç­‰å…³é”®ç»“æ„è·å¾—æ›´å¤šæ³¨æ„åŠ›
- ä¸åŒæ•°å­—ç±»åˆ«çš„æ³¨æ„åŠ›æ¨¡å¼æœ‰æ˜æ˜¾å·®å¼‚

## âš™ï¸ æœ€ä¼˜æ¨¡å‹é…ç½®

åŸºäºå®éªŒç»“æœçš„æœ€ä¼˜é…ç½®ï¼š
```python
{
    'img_size': 28,        # å›¾åƒå°ºå¯¸
    'patch_size': 7,       # Patchå¤§å°
    'in_channels': 1,      # è¾“å…¥é€šé“æ•°
    'num_classes': 10,     # ç±»åˆ«æ•°
    'embed_dim': 128,      # åµŒå…¥ç»´åº¦ï¼ˆå…³é”®å‚æ•°ï¼‰
    'num_heads': 4,        # æ³¨æ„åŠ›å¤´æ•°
    'num_layers': 6,       # Transformerå±‚æ•°
    'mlp_dim': 256,        # MLPç»´åº¦
    'dropout': 0.1         # Dropoutç‡
}
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **æ¨¡å‹åŠ è½½å¤±è´¥**: å·¥å…·ä¼šè‡ªåŠ¨ä½¿ç”¨éšæœºæƒé‡è¿›è¡Œæ¼”ç¤º
2. **CUDAå†…å­˜ä¸è¶³**: è®¾ç½® `device='cpu'`
3. **ä¾èµ–åº“ç¼ºå¤±**: è¿è¡Œ `pip install torch torchvision matplotlib seaborn opencv-python`

### æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨GPUåŠ é€Ÿï¼š`device='cuda'`
- å‡å°‘æ ·æœ¬æ•°é‡ï¼š`num_samples=3`
- é™ä½å›¾åƒåˆ†è¾¨ç‡ï¼šä¸å»ºè®®ï¼Œä¼šå½±å“patchåˆ’åˆ†

## ğŸ“ˆ æ‰©å±•ä½¿ç”¨

### è‡ªå®šä¹‰åˆ†æ
```python
class CustomAnalyzer(AttentionVisualizer):
    def custom_attention_analysis(self, image, label):
        # æ·»åŠ è‡ªå®šä¹‰åˆ†æé€»è¾‘
        pass
```

### æ‰¹é‡æ¨¡å‹å¯¹æ¯”
```python
models = ["model1.pth", "model2.pth", "model3.pth"]
for model_path in models:
    visualizer = AttentionVisualizer(model_path)
    visualizer.run_comprehensive_analysis(model_path)
```

## ğŸ“ æ•™å­¦ä»·å€¼

è¿™ä¸ªå¯è§†åŒ–å·¥å…·ç‰¹åˆ«é€‚åˆï¼š
1. **ç†è§£Transformeræœºåˆ¶**: ç›´è§‚å±•ç¤ºè‡ªæ³¨æ„åŠ›çš„å·¥ä½œåŸç†
2. **åˆ†ææ¨¡å‹è¡Œä¸º**: ç†è§£æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹
3. **è°ƒè¯•æ¨¡å‹è®¾è®¡**: å‘ç°æ½œåœ¨çš„æ³¨æ„åŠ›æ¨¡å¼é—®é¢˜
4. **æ•™å­¦æ¼”ç¤º**: ä¸ºå­¦ç”Ÿå±•ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶

## ğŸ“š å‚è€ƒèµ„æº

- **å®Œæ•´é¡¹ç›®æ–‡æ¡£**: `docs/vision_transformer_mnist.md`
- **è¯¦ç»†ä½¿ç”¨è¯´æ˜**: `README_attention_visualization.md`
- **æ ¸å¿ƒä»£ç **: `attention_visualizer.py`
- **æµ‹è¯•è„šæœ¬**: `test_attention_visualizer.py`
- **æ¼”ç¤ºè„šæœ¬**: `demo_attention_visualization.py`

---

**æç¤º**: è™½ç„¶ç¤ºä¾‹ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼Œä½†æ³¨æ„åŠ›å¯è§†åŒ–çš„æ–¹æ³•å’Œå·¥å…·å®Œå…¨é€‚ç”¨äºè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå®é™…è®­ç»ƒæ¨¡å‹ä¼šå±•ç°å‡ºæ›´æœ‰æ„ä¹‰çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚ 