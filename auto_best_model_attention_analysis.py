#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æœ€ä½³æ¨¡å‹æ³¨æ„åŠ›åˆ†æè„šæœ¬

è¯¥è„šæœ¬è‡ªåŠ¨ï¼š
1. è¯»å–å®éªŒç»“æœï¼Œæ‰¾åˆ°æœ€ä½³æ¨¡å‹
2. åŠ è½½çœŸå®è®­ç»ƒçš„æ¨¡å‹æƒé‡
3. è¿›è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–åˆ†æ
4. ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python auto_best_model_attention_analysis.py
"""

import os
import json
import sys
import torch
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
from attention_visualizer import AttentionVisualizer

class BestModelAnalyzer:
    """æœ€ä½³æ¨¡å‹è‡ªåŠ¨åˆ†æå™¨"""
    
    def __init__(self, experiment_dir="exp/experiments"):
        self.experiment_dir = experiment_dir
        
    def find_latest_experiment(self):
        """æ‰¾åˆ°æœ€æ–°çš„å®éªŒç›®å½•"""
        experiment_path = Path(self.experiment_dir)
        if not experiment_path.exists():
            raise FileNotFoundError(f"å®éªŒç›®å½•ä¸å­˜åœ¨: {self.experiment_dir}")
        
        # æ‰¾åˆ°æ‰€æœ‰å®éªŒç›®å½•
        exp_dirs = [d for d in experiment_path.iterdir() if d.is_dir()]
        if not exp_dirs:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®éªŒç›®å½•")
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œå–æœ€æ–°çš„
        latest_exp = sorted(exp_dirs, key=lambda x: x.name)[-1]
        print(f"ğŸ” æ‰¾åˆ°æœ€æ–°å®éªŒç›®å½•: {latest_exp.name}")
        return latest_exp
    
    def parse_experiment_results(self, exp_dir):
        """è§£æå®éªŒç»“æœï¼Œæ‰¾åˆ°æœ€ä½³æ¨¡å‹"""
        # è¯»å–å®éªŒæŠ¥å‘Š
        report_path = exp_dir / "experiment_report.md"
        if not report_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å®éªŒç»“æœæ–‡ä»¶: {report_path}")
        
        # ä»æŠ¥å‘Šä¸­æå–æœ€ä½³æ¨¡å‹ä¿¡æ¯
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£ææœ€ä½³å®éªŒåç§°
        lines = content.split('\n')
        best_experiment = None
        best_accuracy = 0
        
        for line in lines:
            if "**å®éªŒåç§°**:" in line:
                best_experiment = line.split(":")[-1].strip()
            elif "**æœ€ä½³å‡†ç¡®ç‡**:" in line:
                accuracy_str = line.split(":")[-1].strip().replace('%', '')
                best_accuracy = float(accuracy_str)
                break
        
        if not best_experiment:
            raise ValueError("æ— æ³•ä»å®éªŒæŠ¥å‘Šä¸­è§£ææœ€ä½³æ¨¡å‹ä¿¡æ¯")
        
        print(f"ğŸ“Š æ‰¾åˆ°æœ€ä½³æ¨¡å‹: {best_experiment}")
        print(f"ğŸ¯ æœ€ä½³å‡†ç¡®ç‡: {best_accuracy}%")
        
        return best_experiment, best_accuracy
    
    def locate_best_model_file(self, exp_dir, best_experiment):
        """å®šä½æœ€ä½³æ¨¡å‹æ–‡ä»¶"""
        # æœç´¢æ•´ä¸ªå®éªŒç›®å½•æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        for root, dirs, files in os.walk(exp_dir):
            if "best_model.pth" in files and best_experiment in root:
                model_path = os.path.join(root, "best_model.pth")
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
                return model_path
        
        raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_experiment}")
    
    def get_model_config_from_result(self, exp_dir, best_experiment):
        """ä»å®éªŒç»“æœä¸­è·å–çœŸå®çš„æ¨¡å‹é…ç½®"""
        # æœç´¢result.jsonæ–‡ä»¶
        for root, dirs, files in os.walk(exp_dir):
            if "result.json" in files and best_experiment in root:
                result_file = os.path.join(root, "result.json")
                
                with open(result_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                if 'model_config' in result_data:
                    print(f"ğŸ“‹ ä»å®éªŒç»“æœä¸­è¯»å–çœŸå®æ¨¡å‹é…ç½®: {result_file}")
                    return result_data['model_config']
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆä½†è°ƒæ•´mlp_dimä¸º4å€embed_dimï¼‰
        print("âš ï¸  æœªæ‰¾åˆ°å®éªŒé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨æ¨æ–­é…ç½®")
        model_config = {
            'img_size': 28,
            'patch_size': 7,
            'in_channels': 1,
            'num_classes': 10,
            'embed_dim': 128,
            'num_heads': 4,
            'num_layers': 6,
            'mlp_dim': 512,  # ä¿®æ­£ä¸º4å€embed_dim
            'dropout': 0.1
        }
        
        # å¦‚æœå®éªŒåç§°åŒ…å«embed_dimä¿¡æ¯ï¼Œæ›´æ–°é…ç½®
        if "embed_dim_128" in best_experiment:
            model_config['embed_dim'] = 128
            model_config['mlp_dim'] = 512
        elif "embed_dim_64" in best_experiment:
            model_config['embed_dim'] = 64
            model_config['mlp_dim'] = 256
        elif "embed_dim_32" in best_experiment:
            model_config['embed_dim'] = 32
            model_config['mlp_dim'] = 128
            
        return model_config
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        try:
            print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–æœ€ä½³æ¨¡å‹æ³¨æ„åŠ›åˆ†æ...")
            print("=" * 60)
            
            # 1. æ‰¾åˆ°æœ€æ–°å®éªŒ
            latest_exp_dir = self.find_latest_experiment()
            
            # 2. è§£æå®éªŒç»“æœ
            best_experiment, best_accuracy = self.parse_experiment_results(latest_exp_dir)
            
            # 3. å®šä½æ¨¡å‹æ–‡ä»¶
            model_path = self.locate_best_model_file(latest_exp_dir, best_experiment)
            
            # 4. è·å–æ¨¡å‹é…ç½®
            model_config = self.get_model_config_from_result(latest_exp_dir, best_experiment)
            
            print(f"\nğŸ“‹ æ¨¡å‹é…ç½®:")
            for key, value in model_config.items():
                print(f"   {key}: {value}")
            
            # 5. åˆ›å»ºä¸“ç”¨çš„è¾“å‡ºç›®å½•
            output_dir = f"best_model_attention_analysis_{latest_exp_dir.name}"
            os.makedirs(output_dir, exist_ok=True)
            
            # 6. è¿è¡Œæ³¨æ„åŠ›åˆ†æ
            print(f"\nğŸ”¬ å¼€å§‹æ³¨æ„åŠ›å¯è§†åŒ–åˆ†æ...")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            
            # åˆ›å»ºæ³¨æ„åŠ›å¯è§†åŒ–å™¨
            visualizer = AttentionVisualizer(model_path, device='cuda')
            
            # é‡è¦ï¼šä½¿ç”¨çœŸå®çš„æ¨¡å‹é…ç½®åŠ è½½æ¨¡å‹ï¼
            print("âš™ï¸ åŠ è½½çœŸå®è®­ç»ƒçš„æ¨¡å‹é…ç½®...")
            visualizer.load_model_with_config(model_config)
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            visualizer.output_dir = output_dir
            
            # è·å–æµ‹è¯•æ ·æœ¬å¹¶è¿è¡Œåˆ†æ
            print("ğŸ“Š è·å–æµ‹è¯•æ ·æœ¬...")
            samples = visualizer.get_test_samples(num_samples=8)
            
            print(f"ğŸ” åˆ†æ {len(samples)} ä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æ¨¡å¼...")
            
            # åˆ†ææ¯ä¸ªæ ·æœ¬
            for sample_idx, (image, label) in enumerate(samples):
                print(f"   åˆ†ææ ·æœ¬ {sample_idx+1}/{len(samples)} (æ ‡ç­¾: {label})")
                
                # åŸºæœ¬æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–
                visualizer.visualize_attention_patterns(image, label, sample_idx)
                
                # æ³¨æ„åŠ›å¤´åˆ†æ
                visualizer.analyze_attention_heads(image, label, sample_idx)
                
                # å±‚çº§æ³¨æ„åŠ›å¯¹æ¯”
                visualizer.create_layer_comparison(image, label, sample_idx)
            
            # ç»Ÿè®¡åˆ†æ
            print("ğŸ“ˆ åˆ›å»ºæ³¨æ„åŠ›ç»Ÿè®¡åˆ†æ...")
            visualizer.create_attention_statistics(samples)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            print("ğŸ“ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
            visualizer._generate_analysis_report(samples)
            
            print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
            print(f"ğŸ“Š æœ€ä½³æ¨¡å‹: {best_experiment}")
            print(f"ğŸ¯ å‡†ç¡®ç‡: {best_accuracy:.2f}%")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
            
            # 7. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
            self.generate_comparison_report(
                best_experiment, best_accuracy, model_config, output_dir
            )
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_comparison_report(self, best_experiment, best_accuracy, model_config, output_dir):
        """ç”Ÿæˆå¯¹æ¯”åˆ†ææŠ¥å‘Š"""
        report_path = os.path.join(output_dir, "best_model_analysis_summary.md")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# æœ€ä½³æ¨¡å‹æ³¨æ„åŠ›æœºåˆ¶åˆ†ææ€»ç»“æŠ¥å‘Š\n\n")
            f.write(f"## æ¨¡å‹ä¿¡æ¯\n\n")
            f.write(f"- **å®éªŒåç§°**: {best_experiment}\n")
            f.write(f"- **æœ€ä½³å‡†ç¡®ç‡**: {best_accuracy:.2f}%\n")
            f.write(f"- **æ¨¡å‹ç±»å‹**: Vision Transformer\n")
            f.write(f"- **æ•°æ®é›†**: MNIST\n")
            f.write(f"- **å‚æ•°é‡**: {self.calculate_params(model_config):,}\n\n")
            
            f.write(f"## æ¨¡å‹é…ç½®\n\n")
            f.write("```json\n")
            f.write(json.dumps(model_config, indent=2, ensure_ascii=False))
            f.write("\n```\n\n")
            
            f.write("## çœŸå®è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿\n\n")
            f.write("### ä¸éšæœºæƒé‡æ¨¡å‹çš„åŒºåˆ«\n\n")
            f.write("1. **æ›´æ˜ç¡®çš„æ³¨æ„åŠ›æ¨¡å¼**: è®­ç»ƒåçš„æ¨¡å‹å±•ç°å‡ºæ¸…æ™°çš„ç‰¹å¾å…³æ³¨æ¨¡å¼\n")
            f.write("2. **å±‚çº§åŒ–ç‰¹å¾æå–**: ä¸åŒå±‚çš„æ³¨æ„åŠ›å‘ˆç°æ˜æ˜¾çš„å±‚çº§åŒ–ç‰¹å¾\n")
            f.write("3. **ä»»åŠ¡ç›¸å…³çš„æ³¨æ„åŠ›**: æ³¨æ„åŠ›æ›´ä¸“æ³¨äºæ•°å­—è¯†åˆ«ç›¸å…³çš„å…³é”®åŒºåŸŸ\n")
            f.write("4. **æ³¨æ„åŠ›å¤´ä¸“ä¸šåŒ–**: ä¸åŒæ³¨æ„åŠ›å¤´å±•ç°å‡ºæ˜ç¡®çš„åŠŸèƒ½åˆ†å·¥\n\n")
            
            f.write("### ä¸»è¦å‘ç°\n\n")
            f.write("åŸºäºçœŸå®è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†ä»¥ä¸‹å…³é”®æ¨¡å¼ï¼š\n\n")
            f.write("1. **æ•°å­—è½®å»“è¯†åˆ«**: æ¨¡å‹å¼ºçƒˆå…³æ³¨æ•°å­—çš„è¾¹ç•Œå’Œè½®å»“\n")
            f.write("2. **å…³é”®ç‚¹æ£€æµ‹**: è‡ªåŠ¨è¯†åˆ«æ•°å­—çš„å…³é”®ç‰¹å¾ç‚¹å’Œè¿æ¥å¤„\n")
            f.write("3. **èƒŒæ™¯æŠ‘åˆ¶**: æœ‰æ•ˆå¿½ç•¥èƒŒæ™¯å™ªéŸ³ï¼Œä¸“æ³¨äºå‰æ™¯æ•°å­—\n")
            f.write("4. **ç±»åˆ«ç‰¹å¼‚æ€§**: ä¸åŒæ•°å­—ç±»åˆ«å±•ç°å‡ºä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼\n\n")
            
            f.write("## å¯è§†åŒ–æ–‡ä»¶è¯´æ˜\n\n")
            f.write("- `attention_pattern_sample_*.png`: çœŸå®æ¨¡å‹çš„æ³¨æ„åŠ›çƒ­åŠ›å›¾\n")
            f.write("- `attention_heads_sample_*.png`: è®­ç»ƒåçš„å¤šå¤´æ³¨æ„åŠ›åˆ†æ\n")
            f.write("- `layer_comparison_sample_*.png`: å±‚çº§æ³¨æ„åŠ›æ¼”åŒ–ï¼ˆçœŸå®æ¨¡å‹ï¼‰\n")
            f.write("- `attention_statistics.png`: è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›ç»Ÿè®¡ç‰¹å¾\n")
            f.write("- `attention_analysis_report.md`: è¯¦ç»†æŠ€æœ¯åˆ†ææŠ¥å‘Š\n\n")
            
            f.write("---\n")
            f.write(f"**åˆ†ææ—¶é—´**: è‡ªåŠ¨ç”Ÿæˆ\n")
            f.write(f"**åˆ†æå·¥å…·**: Vision Transformer Attention Visualizer\n")
            f.write(f"**æ¨¡å‹æ¥æº**: {best_experiment}\n")
    
    def calculate_params(self, config):
        """ä¼°ç®—æ¨¡å‹å‚æ•°é‡"""
        embed_dim = config['embed_dim']
        num_layers = config['num_layers']
        num_heads = config['num_heads']
        
        # ç®€åŒ–çš„å‚æ•°ä¼°ç®—
        # Patch embedding + position embedding
        patch_embed_params = (config['patch_size'] ** 2) * config['in_channels'] * embed_dim
        pos_embed_params = (28 // config['patch_size']) ** 2 * embed_dim
        
        # Transformer layers
        # Each layer: Multi-head attention + MLP + LayerNorm
        mha_params = 4 * embed_dim * embed_dim  # Q, K, V, O projections
        mlp_params = 2 * embed_dim * config['mlp_dim']  # FC1 + FC2
        ln_params = 2 * embed_dim  # LayerNorm parameters
        
        layer_params = (mha_params + mlp_params + ln_params) * num_layers
        
        # Classification head
        head_params = embed_dim * config['num_classes']
        
        total_params = patch_embed_params + pos_embed_params + layer_params + head_params
        return int(total_params)

def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸ¯ è‡ªåŠ¨åŒ–æœ€ä½³æ¨¡å‹æ³¨æ„åŠ›åˆ†æ")
        print("åŸºäºçœŸå®è®­ç»ƒæƒé‡çš„Vision Transformer")
        print("-" * 60)
        
        analyzer = BestModelAnalyzer()
        success = analyzer.run_analysis()
        
        if success:
            print("\nâœ… è‡ªåŠ¨åŒ–åˆ†ææˆåŠŸå®Œæˆï¼")
            print("\nğŸ”— ç›¸å…³æ–‡ä»¶:")
            print("   - å®Œæ•´é¡¹ç›®æ–‡æ¡£: docs/vision_transformer_mnist.md")
            print("   - æ³¨æ„åŠ›å¯è§†åŒ–è¯´æ˜: README_attention_visualization.md")
            print("   - ä½¿ç”¨æŒ‡å—: æ³¨æ„åŠ›å¯è§†åŒ–ä½¿ç”¨æŒ‡å—.md")
        else:
            print("\nâŒ è‡ªåŠ¨åŒ–åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            sys.exit(1)
    except Exception as e:
        print(f"âŒ ä¸»å‡½æ•°è¿è¡Œé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 